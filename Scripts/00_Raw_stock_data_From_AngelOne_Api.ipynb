{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c028580-1ae9-43b5-b466-67307c70167d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Installing Required Packages\n",
    "#%pip install -r /dbfs/FileStore/tables/requirements_dev.txt\n",
    "%pip install pyotp\n",
    "%pip install logzero\n",
    "%pip install websocket-client    \n",
    "%pip uninstall pycrypto\n",
    "%pip install pycryptodome    \n",
    "%pip install azure-storage-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c37ef42-836e-4217-8734-1ec5013a6cea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "from SmartApi import SmartConnect\n",
    "import pyotp\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac09a03-5c23-4163-be45-cb453c8f49c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating sparksession\n",
    "spark = SparkSession.builder.appName(\"SmartConnect\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"Asia/Kolkata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e7a6bac-7f35-4bc7-8334-d8404b1488e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Passing Cre\n",
    "\"\"\"\n",
    "api_key = dbutils.secrets.get(scope = \"AngelOne-API-Key\", key = \"AngelOne-API-Key\" )\n",
    "client_code = dbutils.secrets.get(scope = \"AngelOne-client-code\", key = \"AngelOne-client-code\")\n",
    "password =dbutils.secrets.get(scope = \"AngelOne-password\",key = \"AngelOne-password\")\n",
    "totp_key = dbutils.secrets.get(scope = \"AngelOne-Totp-key\", key = \"AngelOne-Totp-key\")\n",
    "\"\"\"\n",
    "api_key = dbutils.secrets.get(scope = \"kv-trading-data-analytic\", key = \"AngelOne-API-Key\" )\n",
    "client_code = dbutils.secrets.get(scope = \"kv-trading-data-analytic\", key = \"AngelOne-client-code\")\n",
    "password =dbutils.secrets.get(scope = \"kv-trading-data-analytic\",key = \"AngelOne-password\")\n",
    "totp_key = dbutils.secrets.get(scope = \"kv-trading-data-analytic\", key = \"AngelOne-Totp-key\")\n",
    "# === Step 2: Generate TOTP ===\n",
    "totp = pyotp.TOTP(totp_key).now()\n",
    "\n",
    "# === Step 3: Create connection and login ===\n",
    "obj = SmartConnect(api_key=api_key)\n",
    "data = obj.generateSession(client_code, password, totp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d448026f-6013-4b45-91d5-60ad1e8e3d00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to fetch OHLC data for given inputs, Here i am creating function with symbol,symbol_token, interval, from_date, to_date as parameters\n",
    "#   symbol and symbol_token: to fetch data from particular stock,\n",
    "#   interval : here i am using 1 day but can be fetched for other intervals as well. For more details https://smartapi.angelbroking.com/docs/Historical\n",
    "#   from_date and to_date: to decide range of data we need, here i am fetching 1 year data.\n",
    "\n",
    "def GetHistoricalOHLCData(symbol,symbol_token, interval, from_date, to_date):\n",
    "    historicParam = {\n",
    "        \"exchange\": \"NSE\",\n",
    "        \"symboltoken\": symbol_token,\n",
    "        \"interval\": interval,\n",
    "        \"fromdate\": from_date,\n",
    "        \"todate\": to_date\n",
    "    }\n",
    "    try:\n",
    "        candles = obj.getCandleData(historicParam)['data']\n",
    "        #print(\"Raw candle data:\", candles)\n",
    "\n",
    "        df = pd.DataFrame(candles)\n",
    "        df.columns = ['datetime', 'open', 'high', 'low', 'close', 'volume']\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        # print(df.head())\n",
    "        df[\"Symbol\"] = symbol\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching data:\", e)\n",
    "        return pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c9e7a13-2d64-4a72-90e0-f3f4b6debbfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a dictionary with symbol and symbol_token, will iterate through this dictionary and fetch data for each symbol\n",
    "\n",
    "SymToken_Symbol_pairs = {\n",
    "10585:'MOM30IETF-EQ'\n",
    ",10690:'NIFTYQLITY-EQ'\n",
    ",25851:'VAL30IETF-EQ'\n",
    ",23806:'ABSLPSE-EQ'\n",
    ",9168:'UTISXN50-EQ'\n",
    ",2328:'CPSEETF-EQ'\n",
    ",14428:'GOLDBEES-EQ'\n",
    ",18284:'HNGSNGBEES-EQ'\n",
    ",7074:'MAHKTECH-EQ'\n",
    ",11241:'HDFCGROWTH-EQ'\n",
    ",21254:'LOWVOLIETF-EQ'\n",
    ",11255:'HDFCQUAL-EQ'\n",
    ",3001:'BSE500IETF-EQ'\n",
    ",13198:'COMMOIETF-EQ'\n",
    ",12578:'FINIETF-EQ'\n",
    ",10723:'INFRAIETF-EQ'\n",
    ",10676:'MNC-EQ'\n",
    ",19640:'ALPHAETF-EQ'\n",
    ",23855:'MIDSMALL-EQ'\n",
    ",22832:'SMALLCAP-EQ'\n",
    ",19237:'MONIFTY500-EQ'\n",
    ",23184:'MOREALTY-EQ'\n",
    ",23181:'MOSMALL250-EQ'\n",
    ",10825:'MOVALUE-EQ'\n",
    ",7422:'MONQ50-EQ'\n",
    ",22739:'MON100-EQ'\n",
    ",24081:'TOP100CASE-EQ'\n",
    ",10576:'NIFTYBEES-EQ'\n",
    ",25606:'MOMENTUM50-EQ'\n",
    ",7412:'ALPHA-EQ'\n",
    ",22344:'ALPL30IETF-EQ'\n",
    ",7844:'AUTOIETF-EQ'\n",
    ",11439:'BANKBEES-EQ'\n",
    ",2636:'DIVOPPBEES-EQ'\n",
    ",24461:'EVINDIA-EQ'\n",
    ",5220:'BFSI-EQ'\n",
    ",5306:'FMCGIETF-EQ'\n",
    ",6297:'HEALTHY-EQ'\n",
    ",10508:'MOHEALTH-EQ'\n",
    ",2435:'CONSUMBEES-EQ'\n",
    ",24944:'MODEFENCE-EQ'\n",
    ",8882:'TNIDETF-EQ'\n",
    ",7979:'MAKEINDIA-EQ'\n",
    ",19084:'ITBEES-EQ'\n",
    ",24861:'METALIETF-EQ'\n",
    ",21423:'MOM100-EQ'\n",
    ",8413:'MIDCAPETF-EQ'\n",
    ",7456:'MIDQ50ADD-EQ'\n",
    ",8077:'MIDCAP-EQ'\n",
    ",4529:'NEXT50IETF-EQ'\n",
    ",24533:'OILIETF-EQ'\n",
    ",4973:'PHARMABEES-EQ'\n",
    ",11386:'PVTBANIETF-EQ'\n",
    ",15032:'PSUBNKBEES-EQ'\n",
    ",25171:'TOP10ADD-EQ'\n",
    ",1200:'ESG-EQ'\n",
    ",17475:'NV20IETF-EQ'\n",
    ",25080:'MULTICAP-EQ'\n",
    ",25996:'EMULTIMQ-EQ'\n",
    ",3507:'MAFANG-EQ'\n",
    ",5782:'MASPTOP50-EQ'\n",
    ",522:'ICICIB22-EQ'\n",
    ",17702:'MIDSELIETF-EQ'\n",
    ",8080:'SILVERBEES-EQ'\n",
    ",4378:'SENSEXIETF-EQ'\n",
    ",17044:'SHARIABEES-EQ'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd0ab43-5366-4d5c-b967-10029a35d9e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dynamic Calculation of FromDate and ToDate to pass to the above function and fetch 1 year data \n",
    "from datetime import date, timedelta, datetime\n",
    "ToDate = date.today()\n",
    "FromDate = ToDate - timedelta(days=365)\n",
    "\n",
    "# Converting Date to datetime\n",
    "FromDate_formatted = f\"{FromDate} 00:00\"  \n",
    "ToDate_formatted = f\"{ToDate} 00:00\"  \n",
    "print(FromDate_formatted, ToDate_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d6bfa3c-049e-4361-b84e-45961395fa52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating pyspark empty df and will append the data from each symbol to it by converting it into pyspark df\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Open\", IntegerType(), True),\n",
    "    StructField(\"High\", IntegerType(), True),\n",
    "    StructField(\"Low\", IntegerType(), True),\n",
    "    StructField(\"Close\", IntegerType(), True),\n",
    "    StructField(\"Volume\", IntegerType(), True),\n",
    "    StructField(\"Symbol\", StringType(), True)\n",
    "])\n",
    "final_df = spark.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e1f7425-e1cb-4f13-b795-d0cc33198eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DailyData = GetHistoricalOHLCData(symbol,token,\"ONE_DAY\",FromDate_formatted,ToDate_formatted)\n",
    "my_df = pd.DataFrame(DailyData)\n",
    "#print(DailyData)\n",
    "print(my_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a78790cf-db43-4d2e-bfb5-ca1a0ae9a12c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Iterating through each symbol in the dictionaty to fetch 1 year data from each symbol and appending all to 1 df (final_df)\n",
    "import time\n",
    "interval = \"ONE_DAY\"\n",
    "# final_df = pd.DataFrame()\n",
    "for token, symbol in SymToken_Symbol_pairs.items():\n",
    "    DailyData = GetHistoricalOHLCData(symbol,token,\"ONE_DAY\",FromDate_formatted,ToDate_formatted)\n",
    "    my_df = pd.DataFrame(DailyData)\n",
    "#    final_df = pd.concat([final_df, my_df], ignore_index=True)\n",
    "#    time.sleep(1)\n",
    "#    print(final_df)\n",
    "    spark_df = spark.createDataFrame(my_df)\n",
    "    final_df = final_df.union(spark_df )\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db6a6382-8dde-46c4-aff6-129825224fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e08771f-5109-44a8-b447-2fd571d393fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtering for friday date, because to check if it is giving proper data or not because of timezones \n",
    "final_df.filter((col(\"symbol\") == 'MOM30IETF-EQ') & (col(\"date\")== \"2025-03-07 00:00\")).orderBy(col(\"date\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d2bc03a-d84b-40d7-a0dc-e96eff881de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_data_path = \"/mnt/rawdata/\"\n",
    "final_df.write.mode(\"overwrite\").format(\"csv\").option(\"header\",\"True\").save(raw_data_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_Raw_stock_data_From_AngelOne_Api",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
